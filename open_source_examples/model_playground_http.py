"""
Models playground implementation using Ollama for conversation detection.
This serves as a starter example for implementing open-source models.

Requirements:
- Ollama installed (https://ollama.ai)
- model pulled from Ollama
"""

import os
import json
import csv
import logging
import argparse
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path

import requests

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Constants
OLLAMA_DEFAULT_PORT = 11434
OLLAMA_BASE_URL = f"http://localhost:{os.environ.get('OLLAMA_PORT', OLLAMA_DEFAULT_PORT)}"
MODEL_NAME = "phi4"
MODEL_ID = "phi4"  # ID used in output filenames
BATCH_SIZE = 25  # Process messages in smaller batches due to context window limitations

def generate_output_filename(input_file: str) -> str:
    """Generate the output filename following the convention:
    labels_[YYYYMMDD_HHMMSS]_[model_id]_[group_name].csv
    """
    # Extract group name from input path
    input_path = Path(input_file)
    group_name = input_path.parent.name

    # Generate timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Construct filename
    filename = f"labels_{timestamp}_{MODEL_ID}_{group_name}.csv"

    # Construct full path in the same directory as input
    output_path = input_path.parent / filename

    return str(output_path)

class Message:
    def __init__(self, id: str, text: str, timestamp: str, user: Dict[str, str]):
        self.id = id
        self.text = text
        self.timestamp = timestamp
        self.user = user

class Label:
    def __init__(self, message_id: str, conversation_id: str, topic: str, timestamp: str, metadata: Dict[str, Any]):
        self.message_id = message_id
        self.conversation_id = conversation_id
        self.topic = topic
        self.timestamp = timestamp
        self.metadata = metadata

class MistralConversationDetector:
    """
    Conversation detector through Ollama for message analysis and topic detection.
    """

    def __init__(self, port=None):
        """Initialize detector."""
        self.model = MODEL_NAME
        self.labeler_id = MODEL_ID
        self.port = port or os.environ.get('OLLAMA_PORT', OLLAMA_DEFAULT_PORT)
        self.base_url = f"http://localhost:{self.port}"

        # Verify Ollama is running
        try:
            # Check server status
            response = requests.post(f"{self.base_url}/api/chat", json={
                "model": self.model,
                "messages": [{"role": "system", "content": "test"}]
            }, timeout=10
            )

            if response.status_code == 404:
                # Try older API format
                response = requests.post(f"{self.base_url}/api/generate", json={
                    "model": self.model,
                    "prompt": "test"
                })

            if response.status_code not in [200, 404]:
                raise ConnectionError(f"Ollama server is not responding properly on port {self.port}")

        except requests.exceptions.ConnectionError:
            raise ConnectionError(f"Ollama server is not running on port {self.port}. Please start Ollama first.")

    def load_messages(self, input_file: str) -> List[Message]:
        """Load messages from CSV file."""
        messages = []
        with open(input_file, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                user = {
                    'username': row['username'],
                    'first_name': row.get('first_name', ''),
                    'last_name': row.get('last_name', '')
                }
                message = Message(
                    id=row['id'],
                    text=row['text'],
                    timestamp=row['timestamp'],
                    user=user
                )
                messages.append(message)
        return messages

    def save_labels(self, labels: List[Label], output_file: str):
        """Save labels to CSV file."""
        output_dir = os.path.dirname(output_file) if os.path.dirname(output_file) else '.'
        os.makedirs(output_dir, exist_ok=True)

        with open(output_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['message_id', 'conversation_id', 'topic', 'timestamp', 'labeler_id', 'confidence'])
            for label in labels:
                writer.writerow([
                    label.message_id,
                    label.conversation_id,
                    label.topic,
                    label.timestamp,
                    label.metadata['labeler_id'],
                    label.metadata['confidence']
                ])

    def detect(self, messages: List[Message]) -> List[Label]:
        """
        Detect conversations in a list of messages.

        Args:
            messages: List of messages to analyze

        Returns:
            List of labels assigning messages to conversations
        """
        message_dicts = [
            {
                'id': msg.id,
                'text': msg.text,
                'timestamp': msg.timestamp,
                'user': msg.user
            }
            for msg in messages
        ]

        all_labels = []
        for i in range(0, len(message_dicts), BATCH_SIZE):
            batch = message_dicts[i:i + BATCH_SIZE]
            logger.info(f"Processing batch {i//BATCH_SIZE + 1} of {(len(message_dicts) + BATCH_SIZE - 1)//BATCH_SIZE}")
            batch_labels = self._detect_conversations(batch)
            all_labels.extend(batch_labels)

        return all_labels

    def _detect_conversations(self, messages: List[Dict[str, Any]]) -> List[Label]:
        """Implementation of conversation detection using model."""
        try:
            prompt = self._create_analysis_prompt(messages)

            # Try new chat API first
            try:
                response = requests.post(
                    f"{self.base_url}/api/chat",
                    json={
                        "model": self.model,
                        "messages": [
                            {
                                "role": "system",
                                "content": """You are a conversation detection system. Output ONLY CSV data in this exact format:
message_id,conversation_id,topic,timestamp,confidence

Each field should be:
- message_id: integer
- conversation_id: integer (0 for spam)
- topic: string without quotes
- timestamp: ISO format string
- confidence: float between 0 and 1

No headers, no quotes, no additional text."""
                            },
                            {"role": "user", "content": prompt}
                        ],
                        "stream": False,
                        "options": {
                            "temperature": 0.3
                        }
                    }, timeout=60
                )
            except:
                # Fallback to older generate API
                response = requests.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.3,
                            "num_predict": 1000,
                        }
                    }
                )

            if response.status_code != 200:
                raise Exception(f"Ollama API error: {response.text}")

            # Parse the response based on API version
            if "message" in response.json():
                # New chat API
                csv_response = response.json()["message"]["content"].strip()
            else:
                # Old generate API
                csv_response = response.json()["response"].strip()

            # Remove any markdown formatting
            if csv_response.startswith('```csv'):
                csv_response = csv_response[6:]
            if csv_response.endswith('```'):
                csv_response = csv_response[:-3]

            # Split into lines and remove empty lines
            lines = [line.strip() for line in csv_response.split('\n') if line.strip()]

            # Process lines into Label objects
            labels = []

            for line in lines:
                try:
                    # Skip header lines and explanatory text
                    if line.startswith('message_id,') or not ',' in line or line.startswith('Note') or line.startswith('Here'):
                        continue

                    # Split and clean the fields
                    parts = [p.strip().strip('"').strip("'") for p in line.split(',')]

                    # Handle both 5-field (no labeler_id) and 6-field formats
                    if len(parts) == 5:
                        msg_id, conv_id, topic, timestamp, confidence = parts
                        labeler_id = self.labeler_id
                    elif len(parts) == 6:
                        msg_id, conv_id, topic, timestamp, labeler_id, confidence = parts
                    else:
                        logger.warning(f"Skipping malformed line (wrong number of fields): {line}")
                        continue

                    try:
                        # Ensure numeric fields are valid
                        msg_id = str(int(msg_id))  # Should be a valid integer
                        conv_id = str(int(conv_id))  # Should be a valid integer
                        confidence = float(confidence)

                        # Ensure labeler_id is always model_id
                        if labeler_id.isdigit():
                            labeler_id = self.labeler_id

                        # Ensure confidence is between 0 and 1
                        confidence = min(1.0, max(0.0, float(confidence)))
                    except ValueError:
                        logger.warning(f"Skipping line with invalid numeric fields: {line}")
                        continue

                    label = Label(
                        message_id=msg_id,
                        conversation_id=conv_id,
                        topic=topic,
                        timestamp=timestamp,
                        metadata={
                            'labeler_id': labeler_id,
                            'confidence': confidence
                        }
                    )
                    labels.append(label)
                except Exception as e:
                    logger.warning(f"Error processing line: {line} - {str(e)}")
                    continue

            return labels

        except Exception as e:
            logger.error(f"Error in Model conversation detection: {str(e)}")
            raise

    def _create_analysis_prompt(self, messages: List[Dict[str, Any]]) -> str:
        """Create the analysis prompt for Model."""
        # Load the conversation detection prompt template
        prompt_path = os.path.join(os.path.dirname(__file__), '..', 'conversation_detection_prompt.txt')
        with open(prompt_path, 'r') as f:
            prompt_template = f.read()

        # Format messages
        formatted_messages = "\n"
        for msg in messages:
            formatted_messages += f"Message ID: {msg['id']}\n"
            formatted_messages += f"Timestamp: {msg['timestamp']}\n"
            formatted_messages += f"User: {msg['user']['username']}\n"
            formatted_messages += f"Content: {msg['text']}\n\n"

        return prompt_template.replace("[MESSAGES]", formatted_messages)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Run model conversation detection on a messages file')
    parser.add_argument('input_file', help='Path to input CSV file containing messages')
    parser.add_argument('--output', '-o', help='Path to output file (optional, will use standard naming convention if not provided)',
                      default=None)
    parser.add_argument('--port', '-p', type=int, help='Ollama server port (default: 11434)',
                      default=OLLAMA_DEFAULT_PORT)

    args = parser.parse_args()

    # Generate output path if not provided
    output_path = args.output or generate_output_filename(args.input_file)

    # Create detector instance with specified port
    detector = MistralConversationDetector(port=args.port)

    # Load messages
    messages = detector.load_messages(args.input_file)
    logger.info(f"Loaded {len(messages)} messages from {args.input_file}")

    # Detect conversations
    labels = detector.detect(messages)

    # Save results
    detector.save_labels(labels, output_path)
    logger.info(f"Results written to: {output_path}")
